import os
import json
import logging
import enum
from typing import Dict, Any, List, Tuple, Optional, Union

import openai
import requests

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('VolcanoAPI')

# å®šä¹‰ API æ¨¡å¼æšä¸¾
class APIMode(enum.Enum):
    OPENAPI = "OpenAPI"
    REST_API = "REST API"

class VolcanoChat:
    """
    ç«å±±å¼•æ“ LLM API å®¢æˆ·ç«¯ç±»
    ç”¨äºå¤„ç†ä¸ç«å±±å¼•æ“ API çš„é€šä¿¡
    """
    def __init__(self, endpoint_id: str, api_key: str, base_url: str, api_mode: APIMode = APIMode.OPENAPI):
        self.endpoint_id = endpoint_id
        self.api_key = api_key
        self.base_url = base_url
        self.api_mode = api_mode
        
        # å¦‚æœæ˜¯ OpenAPI æ¨¡å¼ï¼Œåˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        if api_mode == APIMode.OPENAPI:
            self.client = openai.OpenAI(
                api_key=api_key,
                base_url=base_url
            )
        
        logger.info(f"åˆå§‹åŒ– VolcanoChat å®¢æˆ·ç«¯: endpoint_id={endpoint_id}, base_url={base_url}, api_mode={api_mode.value}")

    def generate(self, prompt: str, system_prompt: str = "", max_tokens: int = 1024, 
                temperature: float = 0.7, top_p: float = 0.9, 
                stop: Optional[List[str]] = None) -> Tuple[str, Dict[str, Any]]:
        """
        ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            system_prompt: ç³»ç»Ÿæç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: é‡‡æ ·å‚æ•°
            stop: åœæ­¢åºåˆ—
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å’Œå®Œæ•´çš„å“åº”ä¿¡æ¯
        """
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            logger.info(f"å‘é€è¯·æ±‚åˆ°ç«å±±å¼•æ“ API: endpoint_id={self.endpoint_id}, api_mode={self.api_mode.value}")
            
            # æ ¹æ® API æ¨¡å¼é€‰æ‹©ä¸åŒçš„è¯·æ±‚æ–¹å¼
            if self.api_mode == APIMode.OPENAPI:
                # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯å‘é€è¯·æ±‚
                response = self.client.chat.completions.create(
                    model=self.endpoint_id,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    stop=stop
                )
                
                # æå–ç”Ÿæˆçš„æ–‡æœ¬
                generated_text = response.choices[0].message.content
                
                # æ„å»ºå“åº”ä¿¡æ¯
                response_info = {
                    "finish_reason": response.choices[0].finish_reason,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    }
                }
            else:  # REST API æ¨¡å¼
                # æ„å»ºè¯·æ±‚ä½“
                request_data = {
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p
                }
                if stop:
                    request_data["stop"] = stop
                
                # å‘é€ REST API è¯·æ±‚
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                # æ„å»ºå®Œæ•´çš„ URL
                url = f"{self.base_url}/v1/completions"
                if not url.startswith("http"):
                    url = f"https://{url}"
                
                response = requests.post(url, headers=headers, json=request_data)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                response.raise_for_status()
                
                # è§£æå“åº”
                response_json = response.json()
                
                # æå–ç”Ÿæˆçš„æ–‡æœ¬
                generated_text = response_json.get("choices", [{}])[0].get("message", {}).get("content", "")
                
                # æ„å»ºå“åº”ä¿¡æ¯
                response_info = {
                    "finish_reason": response_json.get("choices", [{}])[0].get("finish_reason", ""),
                    "usage": response_json.get("usage", {})
                }
            
            return generated_text, response_info
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"ç«å±±å¼•æ“ API è°ƒç”¨å¤±è´¥: {str(e)}")

    def generate_stream(self, prompt: str, system_prompt: str = "", max_tokens: int = 1024, 
                       temperature: float = 0.7, top_p: float = 0.9, 
                       stop: Optional[List[str]] = None):
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            system_prompt: ç³»ç»Ÿæç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: é‡‡æ ·å‚æ•°
            stop: åœæ­¢åºåˆ—
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬æµ
        """
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            logger.info(f"å‘é€æµå¼è¯·æ±‚åˆ°ç«å±±å¼•æ“ API: endpoint_id={self.endpoint_id}, api_mode={self.api_mode.value}")
            
            # æ ¹æ® API æ¨¡å¼é€‰æ‹©ä¸åŒçš„è¯·æ±‚æ–¹å¼
            if self.api_mode == APIMode.OPENAPI:
                # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯å‘é€æµå¼è¯·æ±‚
                response = self.client.chat.completions.create(
                    model=self.endpoint_id,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    stop=stop,
                    stream=True
                )
                
                return response
            else:  # REST API æ¨¡å¼
                # æ„å»ºè¯·æ±‚ä½“
                request_data = {
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "stream": True
                }
                if stop:
                    request_data["stop"] = stop
                
                # å‘é€ REST API æµå¼è¯·æ±‚
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                # æ„å»ºå®Œæ•´çš„ URL
                url = f"{self.base_url}/v1/completions"
                if not url.startswith("http"):
                    url = f"https://{url}"
                
                response = requests.post(url, headers=headers, json=request_data, stream=True)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                response.raise_for_status()
                
                # è¿”å›å“åº”æµ
                return response
            
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"ç«å±±å¼•æ“ API æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")


class VolcanoLLMLoader:
    """
    ComfyUI ç«å±±å¼•æ“ LLM åŠ è½½å™¨èŠ‚ç‚¹
    ç”¨äºåœ¨ ComfyUI ä¸­åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_mode": (["OpenAPI", "REST API"], {"default": "OpenAPI"}),
                "endpoint_id": ("STRING", {"default": "ep-xxxxxxxx"}),
                "api_key": ("STRING", {"default": ""}),
            },
            "optional": {
                "region": ("STRING", {"default": "cn-beijing"}),
                "custom_base_url": ("STRING", {"default": ""}),
            }
        }
    
    RETURN_TYPES = ("VOLCANO_CHAT",)
    RETURN_NAMES = ("chat",)
    FUNCTION = "load_model"
    CATEGORY = "ğŸŒ‹ç«å±±å¼•æ“/LLM"
    
    def load_model(self, api_mode: str, endpoint_id: str, api_key: str, region: str = "cn-beijing", custom_base_url: str = ""):
        """
        åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹
        
        Args:
            api_mode: API æ¨¡å¼ï¼Œ"OpenAPI" æˆ– "REST API"
            endpoint_id: ç«å±±å¼•æ“ç«¯ç‚¹ ID
            api_key: API å¯†é’¥
            region: åŒºåŸŸï¼Œé»˜è®¤ä¸º cn-beijing
            custom_base_url: è‡ªå®šä¹‰åŸºç¡€ URLï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨
            
        Returns:
            VolcanoChat å®ä¾‹
        """
        try:
            # è½¬æ¢ API æ¨¡å¼ä¸ºæšä¸¾å€¼
            api_mode_enum = APIMode.OPENAPI if api_mode == "OpenAPI" else APIMode.REST_API
            
            # æ„å»ºåŸºç¡€ URL
            if custom_base_url and custom_base_url.strip():
                base_url = custom_base_url.strip()
                # æ£€æŸ¥ URL æ˜¯å¦åŒ…å«å¿…è¦çš„åŸŸåéƒ¨åˆ†
                if "volcengine.com" not in base_url:
                    logger.warning(f"è‡ªå®šä¹‰ URL å¯èƒ½ä¸æ­£ç¡®: {base_url}")
                    if api_mode_enum == APIMode.OPENAPI:
                        logger.info("OpenAPI æ¨¡å¼ä¸‹ï¼Œæ­£ç¡®çš„ URL æ ¼å¼åº”ä¸º: https://ark.[region].volcengine.com/v1")
                    else:
                        logger.info("REST API æ¨¡å¼ä¸‹ï¼Œæ­£ç¡®çš„ URL æ ¼å¼åº”ä¸º: https://open.volcengineapi.com")
            else:
                # æ ¹æ® API æ¨¡å¼é€‰æ‹©é»˜è®¤çš„åŸºç¡€ URL
                if api_mode_enum == APIMode.OPENAPI:
                    base_url = f"https://ark.{region}.volcengine.com/v1"
                else:
                    base_url = "https://open.volcengineapi.com"
            
            # ç¡®ä¿ URL æ ¼å¼æ­£ç¡®
            if not base_url.endswith("/"):
                base_url += "/"
                
            # ç¡®ä¿ URL åŒ…å«åè®®
            if not base_url.startswith("http"):
                base_url = "https://" + base_url
            
            logger.info(f"åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹: endpoint_id={endpoint_id}, region={region}, api_mode={api_mode}")
            
            # åˆ›å»º VolcanoChat å®ä¾‹
            chat = VolcanoChat(endpoint_id, api_key, base_url, api_mode_enum)
            
            # æµ‹è¯•è¿æ¥
            self._test_connection(chat)
            
            return (chat,)
            
        except Exception as e:
            logger.error(f"åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"æ— æ³•åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹: {str(e)}")
    
    def _test_connection(self, chat: VolcanoChat):
        """
        æµ‹è¯•ä¸ç«å±±å¼•æ“ API çš„è¿æ¥
        
        Args:
            chat: VolcanoChat å®ä¾‹
        """
        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚ä»¥æµ‹è¯•è¿æ¥
            logger.info("æµ‹è¯•ä¸ç«å±±å¼•æ“ API çš„è¿æ¥...")
            logger.info(f"ä½¿ç”¨ base_url: {chat.base_url}")
            logger.info(f"ä½¿ç”¨ endpoint_id: {chat.endpoint_id}")
            logger.info(f"ä½¿ç”¨ api_mode: {chat.api_mode.value}")
            
            # æ ¹æ® API æ¨¡å¼é€‰æ‹©ä¸åŒçš„æµ‹è¯•æ–¹å¼
            if chat.api_mode == APIMode.OPENAPI:
                # å°è¯•ä½¿ç”¨ chat.completions.create è¿›è¡Œæµ‹è¯•
                try:
                    # ä½¿ç”¨ä¸€ä¸ªéå¸¸ç®€çŸ­çš„æç¤ºè¿›è¡Œæµ‹è¯•ï¼Œåªæ˜¯ä¸ºäº†éªŒè¯è¿æ¥
                    test_response = chat.client.chat.completions.create(
                        model=chat.endpoint_id,
                        messages=[{"role": "user", "content": "æµ‹è¯•è¿æ¥"}],
                        max_tokens=5
                    )
                    logger.info("æˆåŠŸè¿æ¥åˆ°ç«å±±å¼•æ“ OpenAPI å¹¶è·å¾—å“åº”")
                    return
                except openai.NotFoundError as e:
                    logger.warning(f"ç«¯ç‚¹ ID ä¸å­˜åœ¨: {str(e)}")
                    raise RuntimeError(f"API ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ endpoint_id æ˜¯å¦æ­£ç¡®: {chat.endpoint_id}")
                except openai.AuthenticationError as e:
                    logger.warning(f"è®¤è¯å¤±è´¥: {str(e)}")
                    raise RuntimeError(f"API å¯†é’¥æ— æ•ˆæˆ–æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥ api_key")
                except Exception as e:
                    # å¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
                    logger.warning(f"ä½¿ç”¨ chat.completions.create æµ‹è¯•å¤±è´¥: {str(e)}")
                    logger.info("å°è¯•å¤‡ç”¨æ–¹æ³•æµ‹è¯•è¿æ¥...")
                
                # å¤‡ç”¨æµ‹è¯•æ–¹æ³•ï¼šç›´æ¥è¯·æ±‚ models ç«¯ç‚¹
                response = requests.get(
                    f"{chat.base_url}models",
                    headers={"Authorization": f"Bearer {chat.api_key}"}
                )
            else:  # REST API æ¨¡å¼
                # æ„å»ºæµ‹è¯•è¯·æ±‚
                headers = {
                    "Authorization": f"Bearer {chat.api_key}",
                    "Content-Type": "application/json"
                }
                
                # æ„å»ºå®Œæ•´çš„ URL
                url = f"{chat.base_url}v1/completions"
                if not url.startswith("http"):
                    url = f"https://{url}"
                
                # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
                test_data = {
                    "messages": [{"role": "user", "content": "æµ‹è¯•è¿æ¥"}],
                    "max_tokens": 5
                }
                
                response = requests.post(url, headers=headers, json=test_data)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                logger.warning(f"API è¿æ¥æµ‹è¯•è¿”å›çŠ¶æ€ç : {response.status_code}")
                logger.warning(f"å“åº”å†…å®¹: {response.text}")
                if response.status_code == 401:
                    raise RuntimeError("API å¯†é’¥æ— æ•ˆæˆ–æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥ api_key")
                elif response.status_code == 404:
                    # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    error_msg = f"API ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹é…ç½®:\n"
                    error_msg += f"1. API æ¨¡å¼æ˜¯å¦æ­£ç¡®: å½“å‰æ¨¡å¼ä¸º '{chat.api_mode.value}'\n"
                    error_msg += f"2. base_url æ˜¯å¦å®Œæ•´: å½“å‰å€¼ä¸º '{chat.base_url}'\n"
                    error_msg += f"3. endpoint_id æ˜¯å¦æ­£ç¡®: å½“å‰å€¼ä¸º '{chat.endpoint_id}'\n"
                    
                    if chat.api_mode == APIMode.OPENAPI:
                        error_msg += f"OpenAPI æ¨¡å¼ä¸‹ï¼Œæ­£ç¡®çš„ base_url æ ¼å¼åº”ä¸º: https://ark.cn-beijing.volcengine.com/v1/"
                    else:
                        error_msg += f"REST API æ¨¡å¼ä¸‹ï¼Œæ­£ç¡®çš„ base_url æ ¼å¼åº”ä¸º: https://open.volcengineapi.com/"
                    
                    raise RuntimeError(error_msg)
                else:
                    raise RuntimeError(f"API è¿æ¥æµ‹è¯•å¤±è´¥: HTTP {response.status_code}ï¼Œå“åº”: {response.text[:200]}")
            
            logger.info(f"æˆåŠŸè¿æ¥åˆ°ç«å±±å¼•æ“ {chat.api_mode.value}")
            
        except requests.RequestException as e:
            logger.error(f"æµ‹è¯•è¿æ¥æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"æ— æ³•è¿æ¥åˆ°ç«å±±å¼•æ“ API: {str(e)}")


class VolcanoLLMPrompt:
    """
    ç«å±±å¼•æ“ LLM æç¤ºèŠ‚ç‚¹
    ç”¨äºå‘ç«å±±å¼•æ“ LLM å‘é€æç¤ºå¹¶è·å–å“åº”
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "chat": ("VOLCANO_CHAT",),
                "prompt": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}),
            },
            "optional": {
                "system_prompt": ("STRING", {"multiline": True, "default": ""}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 4096}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.05}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("response", "info")
    FUNCTION = "generate"
    CATEGORY = "ğŸŒ‹ç«å±±å¼•æ“/LLM"
    
    def generate(self, chat, prompt, system_prompt="", max_tokens=1024, temperature=0.7, top_p=0.9):
        """
        ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            chat: VolcanoChat å®ä¾‹
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            system_prompt: ç³»ç»Ÿæç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: é‡‡æ ·å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å’Œå“åº”ä¿¡æ¯çš„ JSON å­—ç¬¦ä¸²
        """
        try:
            response_text, response_info = chat.generate(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
            
            # å°†å“åº”ä¿¡æ¯è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
            info_json = json.dumps(response_info, ensure_ascii=False, indent=2)
            
            return (response_text, info_json)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {str(e)}")


# å¯¼å‡ºèŠ‚ç‚¹ç±»ï¼Œä¾› __init__.py ä½¿ç”¨
__all__ = ["VolcanoLLMLoader", "VolcanoLLMPrompt", "VolcanoChat"]

import os
import json
import logging
import enum
import base64
from typing import Dict, Any, List, Tuple, Optional, Union
from io import BytesIO
from PIL import Image

import openai
import requests

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('VolcanoAPI')

# å®šä¹‰ API æ¨¡å¼æšä¸¾
class APIMode(enum.Enum):
    OPENAPI = "OpenAPI"
    REST_API = "REST API"

# å®šä¹‰å†…å®¹ç±»å‹æšä¸¾
class ContentType(enum.Enum):
    TEXT = "text"
    IMAGE = "image_url"
    VIDEO = "video_url"

class VolcanoChat:
    """
    ç«å±±å¼•æ“ LLM API å®¢æˆ·ç«¯ç±»
    æ”¯æŒå¤šæ¨¡æ€è¾“å…¥è¾“å‡º
    """
    def __init__(self, api_mode: APIMode, base_url: str, api_key: str, endpoint_id: str = None):
        self.api_mode = api_mode
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.endpoint_id = endpoint_id
        
        # å¦‚æœæ˜¯ OpenAPI æ¨¡å¼ï¼Œåˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        if api_mode == APIMode.OPENAPI:
            logger.info(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨base_url={self.base_url}")
            self.client = openai.OpenAI(
                api_key=api_key,
                base_url=self.base_url
            )
            logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        
        logger.info(f"åˆå§‹åŒ– VolcanoChat å®¢æˆ·ç«¯: base_url={base_url}, api_mode={api_mode.value}")

    def _encode_image(self, image_path_or_tensor):
        """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64æ ¼å¼"""
        try:
            if isinstance(image_path_or_tensor, str):
                # æ–‡ä»¶è·¯å¾„
                with open(image_path_or_tensor, "rb") as image_file:
                    return base64.b64encode(image_file.read()).decode('utf-8')
            else:
                # ComfyUI tensoræ ¼å¼
                # å‡è®¾æ˜¯PIL Imageæˆ–numpy array
                if hasattr(image_path_or_tensor, 'save'):
                    # PIL Image
                    buffer = BytesIO()
                    image_path_or_tensor.save(buffer, format='PNG')
                    return base64.b64encode(buffer.getvalue()).decode('utf-8')
                else:
                    # numpy arrayæˆ–tensorï¼Œéœ€è¦è½¬æ¢
                    import numpy as np
                    if hasattr(image_path_or_tensor, 'numpy'):
                        image_array = image_path_or_tensor.numpy()
                    else:
                        image_array = image_path_or_tensor
                    
                    # ç¡®ä¿æ•°ç»„åœ¨0-255èŒƒå›´å†…
                    if image_array.max() <= 1.0:
                        image_array = (image_array * 255).astype(np.uint8)
                    
                    # è½¬æ¢ä¸ºPIL Image
                    if len(image_array.shape) == 4:  # batch dimension
                        image_array = image_array[0]
                    if len(image_array.shape) == 3 and image_array.shape[0] in [1, 3, 4]:  # CHW format
                        image_array = np.transpose(image_array, (1, 2, 0))
                    
                    pil_image = Image.fromarray(image_array)
                    buffer = BytesIO()
                    pil_image.save(buffer, format='PNG')
                    return base64.b64encode(buffer.getvalue()).decode('utf-8')
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç¼–ç å¤±è´¥: {str(e)}")
            raise

    def _prepare_messages(self, content_list: List[Dict[str, Any]], system_prompt: str = ""):
        """å‡†å¤‡å¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
        user_content = []
        
        for content_item in content_list:
            content_type = content_item.get("type", "text")
            
            if content_type == "text":
                user_content.append({
                    "type": "text",
                    "text": content_item["content"]
                })
            elif content_type == "image":
                # å¤„ç†å›¾ç‰‡
                image_data = content_item["content"]
                if isinstance(image_data, str) and (image_data.startswith("http") or image_data.startswith("data:")):
                    # URLæˆ–base64æ ¼å¼
                    image_url = image_data
                else:
                    # éœ€è¦ç¼–ç çš„å›¾ç‰‡
                    encoded_image = self._encode_image(image_data)
                    image_url = f"data:image/png;base64,{encoded_image}"
                
                user_content.append({
                    "type": "image_url",
                    "image_url": {"url": image_url}
                })
            elif content_type == "video":
                # å¤„ç†è§†é¢‘ï¼ˆç±»ä¼¼å›¾ç‰‡å¤„ç†ï¼‰
                video_data = content_item["content"]
                if isinstance(video_data, str) and video_data.startswith("http"):
                    video_url = video_data
                else:
                    # å¯¹äºæœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                    video_url = video_data  # ç®€åŒ–å¤„ç†
                
                user_content.append({
                    "type": "video_url",
                    "video_url": {"url": video_url}
                })
        
        messages.append({"role": "user", "content": user_content})
        return messages

    def generate_multimodal(self, content_list: List[Dict[str, Any]], system_prompt: str = "", 
                           max_tokens: int = 1024, temperature: float = 0.7, top_p: float = 0.9,
                           model: str = None) -> Tuple[str, Dict[str, Any]]:
        """
        å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆ
        """
        try:
            messages = self._prepare_messages(content_list, system_prompt)
            
            if self.api_mode == APIMode.OPENAPI:
                # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯
                model_name = self.endpoint_id or model
                if not model_name:
                    raise ValueError("OpenAPIæ¨¡å¼éœ€è¦åœ¨VolcanoLLMLoaderä¸­æä¾›endpoint_id")
                
                logger.info(f"è°ƒç”¨OpenAPIï¼Œæ¨¡å‹: {model_name}")
                response = self.client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p
                )
                
                generated_text = response.choices[0].message.content
                response_info = {
                    "finish_reason": response.choices[0].finish_reason,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    }
                }
            else:
                # REST APIæ¨¡å¼
                if not model:
                    raise ValueError("REST APIæ¨¡å¼éœ€è¦æä¾›modelå‚æ•°")
                
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                request_data = {
                    "model": model,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p
                }
                
                url = f"{self.base_url}/chat/completions"
                logger.info(f"è°ƒç”¨REST API: {url}")
                response = requests.post(url, headers=headers, json=request_data, timeout=30)
                response.raise_for_status()
                
                response_json = response.json()
                generated_text = response_json.get("choices", [{}])[0].get("message", {}).get("content", "")
                response_info = {
                    "finish_reason": response_json.get("choices", [{}])[0].get("finish_reason", ""),
                    "usage": response_json.get("usage", {})
                }
            
            return generated_text, response_info
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€ç”Ÿæˆå¤±è´¥: {str(e)}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"HTTPçŠ¶æ€ç : {e.response.status_code}")
                logger.error(f"å“åº”å†…å®¹: {e.response.text}")
            raise RuntimeError(f"ç«å±±å¼•æ“APIè°ƒç”¨å¤±è´¥: {str(e)}")


class VolcanoLLMLoader:
    """
    ç«å±±å¼•æ“ LLM åŠ è½½å™¨èŠ‚ç‚¹ - ç®€åŒ–ç‰ˆ
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_mode": (["OpenAPI", "REST API"], {"default": "OpenAPI"}),
                "base_url": ("STRING", {"default": "https://ark.cn-beijing.volcengine.com/api/v3"}),
                "api_key": ("STRING", {"default": ""}),
            },
            "optional": {
                "endpoint_id": ("STRING", {"default": ""}),  # OpenAPIæ¨¡å¼ä½¿ç”¨
            }
        }
    
    RETURN_TYPES = ("VOLCANO_CHAT",)
    RETURN_NAMES = ("chat",)
    FUNCTION = "load_model"
    CATEGORY = "ğŸŒ‹ç«å±±å¼•æ“/LLM"
    
    def load_model(self, api_mode: str, base_url: str, api_key: str, endpoint_id: str = ""):
        try:
            api_mode_enum = APIMode.OPENAPI if api_mode == "OpenAPI" else APIMode.REST_API
            
            # éªŒè¯å¿…éœ€å‚æ•°
            if api_mode_enum == APIMode.OPENAPI and not endpoint_id:
                logger.warning("OpenAPIæ¨¡å¼å»ºè®®æä¾›endpoint_id")
            
            # åˆ›å»ºå®¢æˆ·ç«¯
            chat = VolcanoChat(
                api_mode=api_mode_enum,
                base_url=base_url,
                api_key=api_key,
                endpoint_id=endpoint_id if endpoint_id else None
            )
            
            logger.info(f"æˆåŠŸåˆ›å»ºç«å±±å¼•æ“å®¢æˆ·ç«¯: {api_mode}, {base_url}")
            return (chat,)
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åŠ è½½ç«å±±å¼•æ“æ¨¡å‹: {str(e)}")


class VolcanoMultiModalInput:
    """
    å¤šæ¨¡æ€è¾“å…¥èŠ‚ç‚¹
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "chat": ("VOLCANO_CHAT",),
            },
            "optional": {
                "text_input": ("STRING", {"multiline": True, "default": ""}),
                "image_input": ("IMAGE",),
                "video_input": ("STRING", {"default": ""}),  # è§†é¢‘è·¯å¾„æˆ–URL
                "system_prompt": ("STRING", {"multiline": True, "default": ""}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 4096}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.05}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "IMAGE", "STRING")
    RETURN_NAMES = ("text_output", "info", "image_output", "video_output")
    FUNCTION = "process"
    CATEGORY = "ğŸŒ‹ç«å±±å¼•æ“/å¤šæ¨¡æ€"
    
    def process(self, chat, text_input="", image_input=None, video_input="", 
                system_prompt="", max_tokens=1024, temperature=0.7, top_p=0.9):
        try:
            # æ„å»ºå†…å®¹åˆ—è¡¨
            content_list = []
            
            if text_input:
                content_list.append({"type": "text", "content": text_input})
            
            if image_input is not None:
                content_list.append({"type": "image", "content": image_input})
            
            if video_input:
                content_list.append({"type": "video", "content": video_input})
            
            if not content_list:
                raise ValueError("è‡³å°‘éœ€è¦æä¾›ä¸€ç§è¾“å…¥å†…å®¹")
            
            # è°ƒç”¨API - ç§»é™¤modelå‚æ•°ï¼Œè®©VolcanoChatä½¿ç”¨endpoint_id
            response_text, response_info = chat.generate_multimodal(
                content_list=content_list,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
            
            # å¤„ç†è¾“å‡º
            info_json = json.dumps(response_info, ensure_ascii=False, indent=2)
            
            # ç›®å‰ä¸»è¦è¿”å›æ–‡æœ¬ï¼Œå›¾ç‰‡å’Œè§†é¢‘è¾“å‡ºéœ€è¦æ ¹æ®APIå“åº”æ ¼å¼è°ƒæ•´
            empty_image = None  # éœ€è¦åˆ›å»ºç©ºç™½å›¾ç‰‡tensor
            empty_video = ""
            
            return (response_text, info_json, empty_image, empty_video)
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€å¤„ç†å¤±è´¥: {str(e)}")
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if "Connection error" in str(e):
                raise RuntimeError(f"è¿æ¥å¤±è´¥: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚è¯¦ç»†é”™è¯¯: {str(e)}")
            elif "401" in str(e) or "Unauthorized" in str(e):
                raise RuntimeError(f"è®¤è¯å¤±è´¥: è¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®ã€‚è¯¦ç»†é”™è¯¯: {str(e)}")
            elif "404" in str(e):
                raise RuntimeError(f"ç«¯ç‚¹ä¸å­˜åœ¨: è¯·æ£€æŸ¥endpoint_idæ˜¯å¦æ­£ç¡®ã€‚è¯¦ç»†é”™è¯¯: {str(e)}")
            else:
                raise RuntimeError(f"å¤„ç†å¤±è´¥: {str(e)}")


# ç®€åŒ–çš„æ–‡æœ¬ä¸“ç”¨èŠ‚ç‚¹ï¼ˆå‘åå…¼å®¹ï¼‰
class VolcanoLLMPrompt:
    """
    ç«å±±å¼•æ“æ–‡æœ¬æç¤ºèŠ‚ç‚¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "chat": ("VOLCANO_CHAT",),
                "prompt": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}),
            },
            "optional": {
                "system_prompt": ("STRING", {"multiline": True, "default": ""}),
                "model": ("STRING", {"default": ""}),  # REST APIæ¨¡å¼éœ€è¦
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 4096}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.05}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("response", "info")
    FUNCTION = "generate"
    CATEGORY = "ğŸŒ‹ç«å±±å¼•æ“/LLM"
    
    def generate(self, chat, prompt, system_prompt="", model="", max_tokens=1024, temperature=0.7, top_p=0.9):
        try:
            content_list = [{"type": "text", "content": prompt}]
            
            response_text, response_info = chat.generate_multimodal(
                content_list=content_list,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                model=model
            )
            
            info_json = json.dumps(response_info, ensure_ascii=False, indent=2)
            return (response_text, info_json)
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise RuntimeError(f"ç”Ÿæˆå¤±è´¥: {str(e)}")


# å¯¼å‡ºèŠ‚ç‚¹ç±»
__all__ = ["VolcanoLLMLoader", "VolcanoLLMPrompt", "VolcanoMultiModalInput", "VolcanoChat"]

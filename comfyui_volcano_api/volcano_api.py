import os
import json
import logging
from typing import Dict, Any, List, Tuple, Optional

import openai
import requests

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('VolcanoAPI')

class VolcanoChat:
    """
    ç«å±±å¼•æ“ LLM API å®¢æˆ·ç«¯ç±»
    ç”¨äºå¤„ç†ä¸ç«å±±å¼•æ“ API çš„é€šä¿¡
    """
    def __init__(self, endpoint_id: str, api_key: str, base_url: str):
        self.endpoint_id = endpoint_id
        self.api_key = api_key
        self.base_url = base_url
        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        logger.info(f"åˆå§‹åŒ– VolcanoChat å®¢æˆ·ç«¯: endpoint_id={endpoint_id}, base_url={base_url}")

    def generate(self, prompt: str, system_prompt: str = "", max_tokens: int = 1024, 
                temperature: float = 0.7, top_p: float = 0.9, 
                stop: Optional[List[str]] = None) -> Tuple[str, Dict[str, Any]]:
        """
        ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            system_prompt: ç³»ç»Ÿæç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: é‡‡æ ·å‚æ•°
            stop: åœæ­¢åºåˆ—
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å’Œå®Œæ•´çš„å“åº”ä¿¡æ¯
        """
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            logger.info(f"å‘é€è¯·æ±‚åˆ°ç«å±±å¼•æ“ API: endpoint_id={self.endpoint_id}")
            
            response = self.client.chat.completions.create(
                model=self.endpoint_id,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=stop
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = response.choices[0].message.content
            
            # æ„å»ºå“åº”ä¿¡æ¯
            response_info = {
                "finish_reason": response.choices[0].finish_reason,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            
            return generated_text, response_info
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"ç«å±±å¼•æ“ API è°ƒç”¨å¤±è´¥: {str(e)}")

    def generate_stream(self, prompt: str, system_prompt: str = "", max_tokens: int = 1024, 
                       temperature: float = 0.7, top_p: float = 0.9, 
                       stop: Optional[List[str]] = None):
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            system_prompt: ç³»ç»Ÿæç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: é‡‡æ ·å‚æ•°
            stop: åœæ­¢åºåˆ—
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬æµ
        """
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            logger.info(f"å‘é€æµå¼è¯·æ±‚åˆ°ç«å±±å¼•æ“ API: endpoint_id={self.endpoint_id}")
            
            response = self.client.chat.completions.create(
                model=self.endpoint_id,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=stop,
                stream=True
            )
            
            return response
            
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"ç«å±±å¼•æ“ API æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")


class VolcanoLLMLoader:
    """
    ComfyUI ç«å±±å¼•æ“ LLM åŠ è½½å™¨èŠ‚ç‚¹
    ç”¨äºåœ¨ ComfyUI ä¸­åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "endpoint_id": ("STRING", {"default": "ep-xxxxxxxx"}),
                "api_key": ("STRING", {"default": ""}),
            },
            "optional": {
                "region": ("STRING", {"default": "cn-beijing"}),
                "custom_base_url": ("STRING", {"default": ""}),
            }
        }
    
    RETURN_TYPES = ("VOLCANO_CHAT",)
    RETURN_NAMES = ("chat",)
    FUNCTION = "load_model"
    CATEGORY = "ğŸŒ‹ç«å±±å¼•æ“/LLM"
    
    def load_model(self, endpoint_id: str, api_key: str, region: str = "cn-beijing", custom_base_url: str = ""):
        """
        åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹
        
        Args:
            endpoint_id: ç«å±±å¼•æ“ç«¯ç‚¹ ID
            api_key: API å¯†é’¥
            region: åŒºåŸŸï¼Œé»˜è®¤ä¸º cn-beijing
            custom_base_url: è‡ªå®šä¹‰åŸºç¡€ URLï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨
            
        Returns:
            VolcanoChat å®ä¾‹
        """
        try:
            # æ„å»ºåŸºç¡€ URL
            if custom_base_url:
                base_url = custom_base_url
            else:
                base_url = f"https://ark.{region}.volcengine.com/v1"
            
            # ç¡®ä¿ URL æ ¼å¼æ­£ç¡®
            if not base_url.endswith("/"):
                base_url += "/"
            
            logger.info(f"åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹: endpoint_id={endpoint_id}, region={region}")
            
            # åˆ›å»º VolcanoChat å®ä¾‹
            chat = VolcanoChat(endpoint_id, api_key, base_url)
            
            # æµ‹è¯•è¿æ¥
            self._test_connection(chat)
            
            return (chat,)
            
        except Exception as e:
            logger.error(f"åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"æ— æ³•åŠ è½½ç«å±±å¼•æ“ LLM æ¨¡å‹: {str(e)}")
    
    def _test_connection(self, chat: VolcanoChat):
        """
        æµ‹è¯•ä¸ç«å±±å¼•æ“ API çš„è¿æ¥
        
        Args:
            chat: VolcanoChat å®ä¾‹
        """
        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚ä»¥æµ‹è¯•è¿æ¥
            logger.info("æµ‹è¯•ä¸ç«å±±å¼•æ“ API çš„è¿æ¥...")
            response = requests.get(
                f"{chat.base_url}models",
                headers={"Authorization": f"Bearer {chat.api_key}"}
            )
            
            if response.status_code != 200:
                logger.warning(f"API è¿æ¥æµ‹è¯•è¿”å›çŠ¶æ€ç : {response.status_code}")
                logger.warning(f"å“åº”å†…å®¹: {response.text}")
                if response.status_code == 401:
                    raise RuntimeError("API å¯†é’¥æ— æ•ˆæˆ–æƒé™ä¸è¶³")
                elif response.status_code == 404:
                    raise RuntimeError(f"API ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ region å’Œ endpoint_id æ˜¯å¦æ­£ç¡®")
                else:
                    raise RuntimeError(f"API è¿æ¥æµ‹è¯•å¤±è´¥: HTTP {response.status_code}")
            
            logger.info("æˆåŠŸè¿æ¥åˆ°ç«å±±å¼•æ“ API")
            
        except requests.RequestException as e:
            logger.error(f"æµ‹è¯•è¿æ¥æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"æ— æ³•è¿æ¥åˆ°ç«å±±å¼•æ“ API: {str(e)}")


class VolcanoLLMPrompt:
    """
    ç«å±±å¼•æ“ LLM æç¤ºèŠ‚ç‚¹
    ç”¨äºå‘ç«å±±å¼•æ“ LLM å‘é€æç¤ºå¹¶è·å–å“åº”
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "chat": ("VOLCANO_CHAT",),
                "prompt": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}),
            },
            "optional": {
                "system_prompt": ("STRING", {"multiline": True, "default": ""}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 4096}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.05}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("response", "info")
    FUNCTION = "generate"
    CATEGORY = "ğŸŒ‹ç«å±±å¼•æ“/LLM"
    
    def generate(self, chat, prompt, system_prompt="", max_tokens=1024, temperature=0.7, top_p=0.9):
        """
        ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            chat: VolcanoChat å®ä¾‹
            prompt: ç”¨æˆ·æç¤ºæ–‡æœ¬
            system_prompt: ç³»ç»Ÿæç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: é‡‡æ ·å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å’Œå“åº”ä¿¡æ¯çš„ JSON å­—ç¬¦ä¸²
        """
        try:
            response_text, response_info = chat.generate(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
            
            # å°†å“åº”ä¿¡æ¯è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
            info_json = json.dumps(response_info, ensure_ascii=False, indent=2)
            
            return (response_text, info_json)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            raise RuntimeError(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {str(e)}")


# å¯¼å‡ºèŠ‚ç‚¹ç±»ï¼Œä¾› __init__.py ä½¿ç”¨
__all__ = ["VolcanoLLMLoader", "VolcanoLLMPrompt", "VolcanoChat"]
